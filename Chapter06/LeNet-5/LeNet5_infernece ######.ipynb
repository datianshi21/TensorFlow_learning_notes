{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设定神经网络的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入为28*28的图像[28, 28]\n",
    "INPUT_NODE = 784\n",
    "# 输出为1~10的可能性[10]\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# 图像尺寸\n",
    "IMAGE_SIZE = 28\n",
    "# 图像的颜色通道数，这里只有黑白一种通道\n",
    "NUM_CHANNELS = 1\n",
    "# 标签的数量\n",
    "NUM_LABELS = 10\n",
    "\n",
    "# 第一层卷积的深度\n",
    "CONV1_DEEP = 32\n",
    "# 第一层卷积的过滤器尺寸\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "# 第二层卷积的深度\n",
    "CONV2_DEEP = 64\n",
    "# 第二层卷积的过滤器尺寸\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "# 全连接层的节点个数\n",
    "FC_SIZE = 512\n",
    "\n",
    "\n",
    "# 常见的卷积模型\n",
    "# 本例子卷积模型 输入 -> 卷积层 -> 池化层 -> 卷积层 -> 池化层 -> 全连接层 -> 全连接层\n",
    "# 输入 -> (卷积层+ -> 池化层?)+ -> 全连接层+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义前向传播的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor, train, regularizer):\n",
    "    # 第一层卷积1\n",
    "    # 输入为[x-size=28, y-size=28, channel=1]的图像\n",
    "    # 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=32]\n",
    "    # 过滤器步长=1\n",
    "    # 输出为[x-size=28, y-size=28, deep=32]的矩阵\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        conv1_biases = tf.get_variable(\n",
    "            name=\"bias\", \n",
    "            shape=[CONV1_DEEP], \n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    # 第二层池化1\n",
    "    # 输入为[x-size=28, y-size=28, deep=32]的矩阵\n",
    "    # 过滤器尺寸[x-size=2, y-size=2]\n",
    "    # 过滤器步长=2\n",
    "    # 输出为[x-size=14, y-size=14, deep=32]的矩阵\n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "    # 第三层卷积2\n",
    "    # 输入为[x-size=14, y-size=14, deep=32]的矩阵\n",
    "    # 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=64]\n",
    "    # 过滤器步长=1\n",
    "    # 输出为[x-size=14, y-size=14, deep=64]的矩阵\n",
    "    with tf.variable_scope(\"layer3-conv2\"):\n",
    "        conv2_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        conv2_biases = tf.get_variable(\n",
    "            name=\"bias\", \n",
    "            shape=[CONV2_DEEP], \n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    # 第四层池化2\n",
    "    # 输入为[x-size=14, y-size=14, deep=64]的矩阵\n",
    "    # 过滤器尺寸[x-size=2, y-size=2]\n",
    "    # 过滤器步长=2\n",
    "    # 输出为[x-size=7, y-size=7, deep=64]的矩阵\n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # 把[batch, x-size, y-size, deep]4维矩阵转化为[batch, vector]2维矩阵，长*宽*深度转换为1维向量\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "\n",
    "    # 全连接层    \n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[nodes, FC_SIZE],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        # 只有全连接的权重需要加入正则化\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        # dropout在训练数据的时候，会随机把部分输出改为0\n",
    "        # dropout可以避免过度拟合，dropout一般只在全连接层，而不是在卷积层或者池化层使用\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    # 全连接层\n",
    "    # 输入为[512]的向量\n",
    "    # 输出为[10]的向量\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[FC_SIZE, NUM_LABELS],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 定义神经网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 6000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            NUM_CHANNELS],\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = inference(x,False,regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                IMAGE_SIZE,\n",
    "                IMAGE_SIZE,\n",
    "                NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 主程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s), loss on training batch is 4.10292.\n",
      "After 51 training step(s), loss on training batch is 1.14509.\n",
      "After 101 training step(s), loss on training batch is 0.88744.\n",
      "After 151 training step(s), loss on training batch is 1.03481.\n",
      "After 201 training step(s), loss on training batch is 0.947686.\n",
      "After 251 training step(s), loss on training batch is 0.992111.\n",
      "After 301 training step(s), loss on training batch is 0.828515.\n",
      "After 351 training step(s), loss on training batch is 0.94212.\n",
      "After 401 training step(s), loss on training batch is 0.976691.\n",
      "After 451 training step(s), loss on training batch is 0.928402.\n",
      "After 501 training step(s), loss on training batch is 0.813116.\n",
      "After 551 training step(s), loss on training batch is 0.779961.\n",
      "After 601 training step(s), loss on training batch is 0.767382.\n",
      "After 651 training step(s), loss on training batch is 0.866719.\n",
      "After 701 training step(s), loss on training batch is 0.796455.\n",
      "After 751 training step(s), loss on training batch is 0.791853.\n",
      "After 801 training step(s), loss on training batch is 0.714837.\n",
      "After 851 training step(s), loss on training batch is 0.724097.\n",
      "After 901 training step(s), loss on training batch is 0.692805.\n",
      "After 951 training step(s), loss on training batch is 0.78099.\n",
      "After 1001 training step(s), loss on training batch is 0.706844.\n",
      "After 1051 training step(s), loss on training batch is 0.778876.\n",
      "After 1101 training step(s), loss on training batch is 0.751982.\n",
      "After 1151 training step(s), loss on training batch is 0.729636.\n",
      "After 1201 training step(s), loss on training batch is 0.791294.\n",
      "After 1251 training step(s), loss on training batch is 0.745448.\n",
      "After 1301 training step(s), loss on training batch is 0.717904.\n",
      "After 1351 training step(s), loss on training batch is 0.717133.\n",
      "After 1401 training step(s), loss on training batch is 0.697289.\n",
      "After 1451 training step(s), loss on training batch is 0.736507.\n",
      "After 1501 training step(s), loss on training batch is 0.650605.\n",
      "After 1551 training step(s), loss on training batch is 0.684815.\n",
      "After 1601 training step(s), loss on training batch is 0.734429.\n",
      "After 1651 training step(s), loss on training batch is 0.664986.\n",
      "After 1701 training step(s), loss on training batch is 0.685785.\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
