{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设定神经网络的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入为28*28的图像[28, 28]\n",
    "INPUT_NODE = 784\n",
    "# 输出为1~10的可能性[10]\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# 图像尺寸\n",
    "IMAGE_SIZE = 28\n",
    "# 图像的颜色通道数，这里只有黑白一种通道\n",
    "NUM_CHANNELS = 1\n",
    "# 标签的数量\n",
    "NUM_LABELS = 10\n",
    "\n",
    "# 第一层卷积的深度\n",
    "CONV1_DEEP = 32\n",
    "# 第一层卷积的过滤器尺寸\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "# 第二层卷积的深度\n",
    "CONV2_DEEP = 64\n",
    "# 第二层卷积的过滤器尺寸\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "# 全连接层的节点个数\n",
    "FC_SIZE = 512\n",
    "\n",
    "\n",
    "# 常见的卷积模型\n",
    "# 本例子卷积模型 输入 -> 卷积层 -> 池化层 -> 卷积层 -> 池化层 -> 全连接层 -> 全连接层\n",
    "# 输入 -> (卷积层+ -> 池化层?)+ -> 全连接层+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义前向传播的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor, train, regularizer):\n",
    "    # 第一层卷积1\n",
    "    # 输入为[x-size=28, y-size=28, channel=1]的图像\n",
    "    # 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=32]\n",
    "    # 过滤器步长=1\n",
    "    # 输出为[x-size=28, y-size=28, deep=32]的矩阵\n",
    "    #tf.reset_default_graph()\n",
    "    with tf.variable_scope('layer1-conv1', reuse=True):\n",
    "        conv1_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        conv1_biases = tf.get_variable(\n",
    "            name=\"bias\", \n",
    "            shape=[CONV1_DEEP], \n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "        \n",
    "    # 第二层池化1\n",
    "    # 输入为[x-size=28, y-size=28, deep=32]的矩阵\n",
    "    # 过滤器尺寸[x-size=2, y-size=2]\n",
    "    # 过滤器步长=2\n",
    "    # 输出为[x-size=14, y-size=14, deep=32]的矩阵\n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        \n",
    "    # 第三层卷积2\n",
    "    # 输入为[x-size=14, y-size=14, deep=32]的矩阵\n",
    "    # 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=64]\n",
    "    # 过滤器步长=1\n",
    "    # 输出为[x-size=14, y-size=14, deep=64]的矩阵\n",
    "    with tf.variable_scope(\"layer3-conv2\"):\n",
    "        conv2_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        conv2_biases = tf.get_variable(\n",
    "            name=\"bias\", \n",
    "            shape=[CONV2_DEEP], \n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "    with tf.variable_scope(\"layer3-conv2\"):\n",
    "        conv2_weights = tf.get_vairable(\n",
    "            name=\"weight\",\n",
    "            shape=[CONV2_SIXE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        \n",
    "    # 第四层池化2\n",
    "    # 输入为[x-size=14, y-size=14, deep=64]的矩阵\n",
    "    # 过滤器尺寸[x-size=2, y-size=2]\n",
    "    # 过滤器步长=2\n",
    "    # 输出为[x-size=7, y-size=7, deep=64]的矩阵\n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # 把[batch, x-size, y-size, deep]4维矩阵转化为[batch, vector]2维矩阵，长*宽*深度转换为1维向量\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "\n",
    "    # 全连接层    \n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[nodes, FC_SIZE],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        # 只有全连接的权重需要加入正则化\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        # dropout在训练数据的时候，会随机把部分输出改为0\n",
    "        # dropout可以避免过度拟合，dropout一般只在全连接层，而不是在卷积层或者池化层使用\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    # 全连接层\n",
    "    # 输入为[512]的向量\n",
    "    # 输出为[10]的向量\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable(\n",
    "            name=\"weight\", \n",
    "            shape=[FC_SIZE, NUM_LABELS],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        )\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 定义神经网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 6000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            NUM_CHANNELS],\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = inference(x,False,regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                IMAGE_SIZE,\n",
    "                IMAGE_SIZE,\n",
    "                NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 主程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s), loss on training batch is 4.19632.\n",
      "After 51 training step(s), loss on training batch is 1.16139.\n",
      "After 101 training step(s), loss on training batch is 0.954467.\n",
      "After 151 training step(s), loss on training batch is 0.977561.\n",
      "After 201 training step(s), loss on training batch is 1.04455.\n",
      "After 251 training step(s), loss on training batch is 0.82772.\n",
      "After 301 training step(s), loss on training batch is 0.806839.\n",
      "After 351 training step(s), loss on training batch is 0.858132.\n",
      "After 401 training step(s), loss on training batch is 0.851507.\n",
      "After 451 training step(s), loss on training batch is 0.823184.\n",
      "After 501 training step(s), loss on training batch is 0.756954.\n",
      "After 551 training step(s), loss on training batch is 0.760334.\n",
      "After 601 training step(s), loss on training batch is 0.839297.\n",
      "After 651 training step(s), loss on training batch is 0.84295.\n",
      "After 701 training step(s), loss on training batch is 0.816697.\n",
      "After 751 training step(s), loss on training batch is 0.745767.\n",
      "After 801 training step(s), loss on training batch is 0.798977.\n",
      "After 851 training step(s), loss on training batch is 0.786814.\n",
      "After 901 training step(s), loss on training batch is 0.693391.\n",
      "After 951 training step(s), loss on training batch is 0.749042.\n",
      "After 1001 training step(s), loss on training batch is 0.712054.\n",
      "After 1051 training step(s), loss on training batch is 0.714125.\n",
      "After 1101 training step(s), loss on training batch is 0.743258.\n",
      "After 1151 training step(s), loss on training batch is 0.728321.\n",
      "After 1201 training step(s), loss on training batch is 0.731628.\n",
      "After 1251 training step(s), loss on training batch is 0.729826.\n",
      "After 1301 training step(s), loss on training batch is 0.747178.\n",
      "After 1351 training step(s), loss on training batch is 0.678306.\n",
      "After 1401 training step(s), loss on training batch is 0.726597.\n",
      "After 1451 training step(s), loss on training batch is 0.677791.\n",
      "After 1501 training step(s), loss on training batch is 0.765259.\n",
      "After 1551 training step(s), loss on training batch is 0.695521.\n",
      "After 1601 training step(s), loss on training batch is 0.690888.\n",
      "After 1651 training step(s), loss on training batch is 0.688622.\n",
      "After 1701 training step(s), loss on training batch is 0.701493.\n",
      "After 1751 training step(s), loss on training batch is 0.688703.\n",
      "After 1801 training step(s), loss on training batch is 0.711415.\n",
      "After 1851 training step(s), loss on training batch is 0.712479.\n",
      "After 1901 training step(s), loss on training batch is 0.688473.\n",
      "After 1951 training step(s), loss on training batch is 0.675563.\n",
      "After 2001 training step(s), loss on training batch is 0.701755.\n",
      "After 2051 training step(s), loss on training batch is 0.650083.\n",
      "After 2101 training step(s), loss on training batch is 0.780547.\n",
      "After 2151 training step(s), loss on training batch is 0.693491.\n",
      "After 2201 training step(s), loss on training batch is 0.684363.\n",
      "After 2251 training step(s), loss on training batch is 0.65291.\n",
      "After 2301 training step(s), loss on training batch is 0.66848.\n",
      "After 2351 training step(s), loss on training batch is 0.743349.\n",
      "After 2401 training step(s), loss on training batch is 0.666548.\n",
      "After 2451 training step(s), loss on training batch is 0.694348.\n",
      "After 2501 training step(s), loss on training batch is 0.6566.\n",
      "After 2551 training step(s), loss on training batch is 0.669019.\n",
      "After 2601 training step(s), loss on training batch is 0.761091.\n",
      "After 2651 training step(s), loss on training batch is 0.784003.\n",
      "After 2701 training step(s), loss on training batch is 0.743174.\n",
      "After 2751 training step(s), loss on training batch is 0.671278.\n",
      "After 2801 training step(s), loss on training batch is 0.799966.\n",
      "After 2851 training step(s), loss on training batch is 0.639771.\n",
      "After 2901 training step(s), loss on training batch is 0.691788.\n",
      "After 2951 training step(s), loss on training batch is 0.699531.\n",
      "After 3001 training step(s), loss on training batch is 0.688107.\n",
      "After 3051 training step(s), loss on training batch is 0.680161.\n",
      "After 3101 training step(s), loss on training batch is 0.645887.\n",
      "After 3151 training step(s), loss on training batch is 0.697043.\n",
      "After 3201 training step(s), loss on training batch is 0.734392.\n",
      "After 3251 training step(s), loss on training batch is 0.645024.\n",
      "After 3301 training step(s), loss on training batch is 0.761334.\n",
      "After 3351 training step(s), loss on training batch is 0.642663.\n",
      "After 3401 training step(s), loss on training batch is 0.651376.\n",
      "After 3451 training step(s), loss on training batch is 0.68711.\n",
      "After 3501 training step(s), loss on training batch is 0.652472.\n",
      "After 3551 training step(s), loss on training batch is 0.704896.\n",
      "After 3601 training step(s), loss on training batch is 0.63451.\n",
      "After 3651 training step(s), loss on training batch is 0.64772.\n",
      "After 3701 training step(s), loss on training batch is 0.627923.\n",
      "After 3751 training step(s), loss on training batch is 0.655801.\n",
      "After 3801 training step(s), loss on training batch is 0.639419.\n",
      "After 3851 training step(s), loss on training batch is 0.725074.\n",
      "After 3901 training step(s), loss on training batch is 0.640694.\n",
      "After 3951 training step(s), loss on training batch is 0.685251.\n",
      "After 4001 training step(s), loss on training batch is 0.645235.\n",
      "After 4051 training step(s), loss on training batch is 0.707476.\n",
      "After 4101 training step(s), loss on training batch is 0.629642.\n",
      "After 4151 training step(s), loss on training batch is 0.675816.\n",
      "After 4201 training step(s), loss on training batch is 0.692207.\n",
      "After 4251 training step(s), loss on training batch is 0.753577.\n",
      "After 4301 training step(s), loss on training batch is 0.654829.\n",
      "After 4351 training step(s), loss on training batch is 0.705157.\n",
      "After 4401 training step(s), loss on training batch is 0.712626.\n",
      "After 4451 training step(s), loss on training batch is 0.708498.\n",
      "After 4501 training step(s), loss on training batch is 0.640304.\n",
      "After 4551 training step(s), loss on training batch is 0.694212.\n",
      "After 4601 training step(s), loss on training batch is 0.641544.\n",
      "After 4651 training step(s), loss on training batch is 0.651943.\n",
      "After 4701 training step(s), loss on training batch is 0.652181.\n",
      "After 4751 training step(s), loss on training batch is 0.66765.\n",
      "After 4801 training step(s), loss on training batch is 0.678398.\n",
      "After 4851 training step(s), loss on training batch is 0.633966.\n",
      "After 4901 training step(s), loss on training batch is 0.676347.\n",
      "After 4951 training step(s), loss on training batch is 0.666642.\n",
      "After 5001 training step(s), loss on training batch is 0.634914.\n",
      "After 5051 training step(s), loss on training batch is 0.635596.\n",
      "After 5101 training step(s), loss on training batch is 0.687717.\n",
      "After 5151 training step(s), loss on training batch is 0.628358.\n",
      "After 5201 training step(s), loss on training batch is 0.706847.\n",
      "After 5251 training step(s), loss on training batch is 0.663235.\n",
      "After 5301 training step(s), loss on training batch is 0.672363.\n",
      "After 5351 training step(s), loss on training batch is 0.650401.\n",
      "After 5401 training step(s), loss on training batch is 0.631488.\n",
      "After 5451 training step(s), loss on training batch is 0.63983.\n",
      "After 5501 training step(s), loss on training batch is 0.649097.\n",
      "After 5551 training step(s), loss on training batch is 0.632884.\n",
      "After 5601 training step(s), loss on training batch is 0.653062.\n",
      "After 5651 training step(s), loss on training batch is 0.641664.\n",
      "After 5701 training step(s), loss on training batch is 0.674871.\n",
      "After 5751 training step(s), loss on training batch is 0.638397.\n",
      "After 5801 training step(s), loss on training batch is 0.633652.\n",
      "After 5851 training step(s), loss on training batch is 0.639394.\n",
      "After 5901 training step(s), loss on training batch is 0.637201.\n",
      "After 5951 training step(s), loss on training batch is 0.649977.\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
